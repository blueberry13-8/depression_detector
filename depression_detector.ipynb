{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Detecting of depression from social network data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this project is to determine how Twitter, a well-known social media site, may be used to monitor users for early signs of depression. The study is inspired by following articles: [Early Detection of Depression: Social Network Analysis and Random Forest Techniques](https://www.jmir.org/2019/6/e12554/), [Depression detection from social network data using machine learning techniques](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6111060/)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "Setup all libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in c:\\users\\gee8w\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import preprocessor as p\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read the data\n",
    "Use data from [Sentiment140 dataset](https://www.kaggle.com/datasets/kazanova/sentiment140)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         target          id                          date     query  \\\n",
      "448282        0  2068921155  Sun Jun 07 14:56:42 PDT 2009  NO_QUERY   \n",
      "1475261       4  2065871668  Sun Jun 07 09:27:21 PDT 2009  NO_QUERY   \n",
      "132529        0  1835774749  Mon May 18 06:43:27 PDT 2009  NO_QUERY   \n",
      "182348        0  1967121891  Fri May 29 19:00:46 PDT 2009  NO_QUERY   \n",
      "907614        4  1695846172  Mon May 04 07:04:29 PDT 2009  NO_QUERY   \n",
      "\n",
      "                   name                                               text  \n",
      "448282    smiley_sophie  my arm still hurts from when i pulled it yeste...  \n",
      "1475261  ImmaChocoholic  I have so much to do outside! Been looking at ...  \n",
      "132529       drmomentum  @AbsolutSara Yes, I knew about the clusterfark...  \n",
      "182348      sweetsheilx  Just woke up and i feel relieved Haha now i ha...  \n",
      "907614        monmariej  LOVING the hot weather forecast for the rest o...  \n",
      "(5087, 6)\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "\n",
    "labels = ['target', 'id', 'date', 'query', 'name', 'text']\n",
    "\n",
    "data = pd.read_csv('data_twitter.csv', names=labels, encoding='ISO-8859-1')\n",
    "\n",
    "sample = data.sample(N, random_state=123)\n",
    "\n",
    "print(sample.head())\n",
    "\n",
    "print(sample[sample['target'] == 0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replace values in 'target' column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "sample['target'] = sample['target'].map({0: 1, 4: 0})\n",
    "sample = sample.drop(['name', 'id', 'date', 'query'], axis=1)\n",
    "X = sample.drop(['target'], axis=1)\n",
    "y = sample['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5087, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 448282 to 113970\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  10000 non-null  int64 \n",
      " 1   text    10000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 234.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sample[sample['target'] == 1].shape)\n",
    "print(sample.info())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get rid of contraction, punctuation, and stop words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "cont_list = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "cont_re = re.compile('(%s)' % '|'.join(cont_list.keys()))\n",
    "\n",
    "def expand_contractions(text):\n",
    "    def replace(match):\n",
    "        return cont_list[match.group(0)]\n",
    "    return cont_re.sub(replace, text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def clean_messages(messages):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    nltk.download('punkt')\n",
    "    cleaned_messages = []\n",
    "    for msg in messages:\n",
    "        msg = str(msg).lower()\n",
    "        msg = BAD_SYMBOLS_RE.sub(' ', msg)\n",
    "        msg = p.clean(msg)\n",
    "\n",
    "        msg = expand_contractions(msg)\n",
    "\n",
    "        msg = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", msg).split())\n",
    "\n",
    "        word_tokens = nltk.tokenize.word_tokenize(msg)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        msg = ' '.join(filtered_sentence)\n",
    "        cleaned_messages.append(msg)\n",
    "\n",
    "    return cleaned_messages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gee8w\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "cleaned_msgs = clean_messages([x for x in X['text']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "tokenizer= Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(cleaned_msgs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "17770"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = tokenizer.texts_to_sequences(cleaned_msgs)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 140)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 140\n",
    "input_tensor = pad_sequences(word_vector, maxlen=MAX_SEQ_LENGTH)\n",
    "input_tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_clf_metrics( y_actual, y_pred ):\n",
    "  print(f'Testing accuracy  = {metrics.accuracy_score(y_actual, y_pred)}')\n",
    "  print(f'Testing precision = {metrics.precision_score(y_actual, y_pred)}')\n",
    "  print(f'Testing recall    = {metrics.recall_score(y_actual, y_pred)}')\n",
    "  print(f'Testing F1-score  = {metrics.f1_score(y_actual, y_pred)}')\n",
    "\n",
    "parameters = [{'penalty' : ['l2'],\n",
    "               'C' : np.logspace(-3, 3, 7),\n",
    "               'solver' : ['newton-cg', 'lbfgs', 'liblinear']},\n",
    "              {'penalty' : ['l1'],\n",
    "               'C' : np.logspace(-3, 3, 7),\n",
    "               'solver' : ['liblinear']}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gee8w\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:416: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "C:\\Users\\gee8w\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\gee8w\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "C:\\Users\\gee8w\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:416: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "C:\\Users\\gee8w\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\gee8w\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "C:\\Users\\gee8w\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Hyperparameters : {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Accuracy : 0.5144\n"
     ]
    }
   ],
   "source": [
    "# Define Grid Search for Logistic Regression with parameters from above\n",
    "gs_clf = GridSearchCV(LogisticRegression(max_iter=1000), parameters)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_tensor, y, random_state=54, stratify=y)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tuned Hyperparameters :\", gs_clf.best_params_)\n",
    "print(\"Accuracy :\", gs_clf.best_score_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy  = 0.5444\n",
      "Testing precision = 0.535887749595251\n",
      "Testing recall    = 0.7806603773584906\n",
      "Testing F1-score  = 0.6355200000000001\n"
     ]
    }
   ],
   "source": [
    "lr_clf = gs_clf.best_estimator_\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "print_clf_metrics(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy  = 0.5116\n",
      "Testing precision = 0.510353227771011\n",
      "Testing recall    = 0.9882075471698113\n",
      "Testing F1-score  = 0.6730923694779116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Declare GNB\n",
    "gauss_nb = GaussianNB()\n",
    "\n",
    "gauss_nb.fit(X_train, y_train)\n",
    "y_pred = gauss_nb.predict(X_test)\n",
    "\n",
    "print_clf_metrics(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LTSM Model (in progress)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}